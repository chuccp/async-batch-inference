# async-batch-inference
用于模型的批量请求
